{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: /Users/ayushgaur/Desktop/NLP/HW3/Data/Raw/IMDB Dataset.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50000,\n",
       "                                               review sentiment\n",
       " 0  One of the other reviewers has mentioned that ...  positive\n",
       " 1  A wonderful little production. <br /><br />The...  positive,\n",
       " PosixPath('/Users/ayushgaur/Desktop/NLP/HW3/Data/processed'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# change the path if required \n",
    "DATA_IN = Path(\"/Users/ayushgaur/Desktop/NLP/HW3/Data/Raw/IMDB Dataset.csv\")\n",
    "\n",
    "OUT_DIR = DATA_IN.parent.parent / \"processed\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Reading:\", DATA_IN)\n",
    "assert DATA_IN.exists(), f\"Not found: {DATA_IN}\"\n",
    "df = pd.read_csv(DATA_IN)\n",
    "assert set(df.columns) == {\"review\", \"sentiment\"}, f\"Unexpected columns: {df.columns.tolist()}\"\n",
    "\n",
    "len(df), df.head(2), OUT_DIR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50-50 split and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(label\n",
       " 0    12526\n",
       " 1    12474\n",
       " Name: count, dtype: int64,\n",
       " label\n",
       " 1    12526\n",
       " 0    12474\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_label(x: str) -> int:\n",
    "    return 1 if str(x).strip().lower() == \"positive\" else 0\n",
    "\n",
    "df_train = df.iloc[:25000].copy().reset_index(drop=True)   \n",
    "df_test  = df.iloc[25000:].copy().reset_index(drop=True)   \n",
    "\n",
    "df_train[\"label\"] = df_train[\"sentiment\"].apply(to_label).astype(int)\n",
    "df_test[\"label\"]  = df_test[\"sentiment\"].apply(to_label).astype(int)\n",
    "\n",
    "df_train[\"label\"].value_counts(), df_test[\"label\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cleaning and tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'other',\n",
       " 'reviewers',\n",
       " 'has',\n",
       " 'mentioned',\n",
       " 'that',\n",
       " 'after',\n",
       " 'watching',\n",
       " 'just',\n",
       " '1',\n",
       " 'oz',\n",
       " 'episode',\n",
       " 'you']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TAG_RE = re.compile(r\"<[^>]+>\")\n",
    "NON_ALNUM_RE = re.compile(r\"[^a-z0-9\\s]\")\n",
    "MULTISPACE_RE = re.compile(r\"\\s+\")\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = html.unescape(str(s))\n",
    "    s = TAG_RE.sub(\" \", s)          \n",
    "    s = s.lower()\n",
    "    s = NON_ALNUM_RE.sub(\" \", s)    \n",
    "    s = MULTISPACE_RE.sub(\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def tokenize(s: str):\n",
    "    return clean_text(s).split()\n",
    "\n",
    "tokenize(df_train.loc[0, \"review\"])[:15]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# building vocab with top 10k frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_vocab(texts, vocab_size=10000):\n",
    "    counter = Counter()\n",
    "    for t in texts:\n",
    "        counter.update(tokenize(t))\n",
    "    word2id = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "    for i, (w, _) in enumerate(counter.most_common(vocab_size - 2), start=2):\n",
    "        word2id[w] = i\n",
    "    id2word = {i: w for w, i in word2id.items()}\n",
    "    return word2id, id2word\n",
    "\n",
    "word2id, id2word = build_vocab(df_train[\"review\"], vocab_size=10000)\n",
    "\n",
    "# Saving vocab\n",
    "(OUT_DIR / \"vocab.json\").write_text(json.dumps(word2id, ensure_ascii=False, indent=2))\n",
    "with open(OUT_DIR / \"vocab.txt\", \"w\") as f:\n",
    "    for i in range(len(id2word)):\n",
    "        f.write(f\"{i}\\t{id2word[i]}\\n\")\n",
    "\n",
    "len(word2id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to IDs; truncating to length 25/50/100; saving CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imdb_test_len100.csv',\n",
       " 'imdb_test_len25.csv',\n",
       " 'imdb_test_len50.csv',\n",
       " 'imdb_train_len100.csv',\n",
       " 'imdb_train_len25.csv',\n",
       " 'imdb_train_len50.csv']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_to_ids(s: str, w2i: dict):\n",
    "    return [w2i.get(tok, 1) for tok in tokenize(s)] \n",
    "\n",
    "def pad_or_truncate(ids, length: int):\n",
    "    return ids[:length] if len(ids) >= length else ids + [0] * (length - len(ids))  \n",
    "\n",
    "def make_dataset(df_in: pd.DataFrame, w2i: dict, length: int):\n",
    "    seqs = [pad_or_truncate(text_to_ids(t, w2i), length) for t in df_in[\"review\"]]\n",
    "    seq_str = [\" \".join(map(str, seq)) for seq in seqs]\n",
    "    return pd.DataFrame({\"ids\": seq_str, \"label\": df_in[\"label\"].astype(int)})\n",
    "\n",
    "for L in [25, 50, 100]:\n",
    "    make_dataset(df_train, word2id, L).to_csv(OUT_DIR / f\"imdb_train_len{L}.csv\", index=False)\n",
    "    make_dataset(df_test, word2id, L).to_csv(OUT_DIR / f\"imdb_test_len{L}.csv\", index=False)\n",
    "\n",
    "sorted(p.name for p in OUT_DIR.glob(\"*.csv\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports and config for the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Core\n",
    "import os, time, math, json, random\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 1337\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Paths\n",
    "ROOT     = Path(\".\").resolve()\n",
    "DATA_DIR = ROOT / \"Data\" / \"processed\"   \n",
    "RESULTS  = ROOT / \"results\"; RESULTS.mkdir(exist_ok=True, parents=True)\n",
    "METRICS_CSV = RESULTS / \"metrics.csv\"\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data loaders for the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImdbCsvDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path: Path):\n",
    "        df = pd.read_csv(path)\n",
    "        self.X = [list(map(int, s.split())) for s in df[\"ids\"].tolist()]\n",
    "        self.y = df[\"label\"].astype(int).tolist()\n",
    "\n",
    "    def __len__(self): return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ids = torch.tensor(self.X[idx], dtype=torch.long)\n",
    "        lab = torch.tensor(self.y[idx], dtype=torch.float32)  \n",
    "        return ids, lab\n",
    "\n",
    "def build_loaders(seq_len: int, batch_size: int = 32):\n",
    "    train_csv = DATA_DIR / f\"imdb_train_len{seq_len}.csv\"\n",
    "    test_csv  = DATA_DIR / f\"imdb_test_len{seq_len}.csv\"\n",
    "\n",
    "    train_ds = ImdbCsvDataset(train_csv)\n",
    "    test_ds  = ImdbCsvDataset(test_csv)\n",
    "\n",
    "    # small validation split from training for loss curves; here 90/10 split\n",
    "    n = len(train_ds)\n",
    "    n_val = int(0.1 * n)\n",
    "    n_train = n - n_val\n",
    "    gen = torch.Generator().manual_seed(SEED)\n",
    "    train_ds, val_ds = torch.utils.data.random_split(train_ds, [n_train, n_val], generator=gen)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  drop_last=False)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVATIONS = {\n",
    "    \"relu\": nn.ReLU(),\n",
    "    \"tanh\": nn.Tanh(),\n",
    "    \"sigmoid\": nn.Sigmoid(),\n",
    "}\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size: int, embed_dim: int = 100, hidden_size: int = 64,\n",
    "                 num_layers: int = 2, dropout: float = 0.5, rnn_type: str = \"lstm\",\n",
    "                 activation: str = \"relu\", bidirectional: bool = False):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "\n",
    "        if rnn_type == \"rnn\":\n",
    "            self.rnn = nn.RNN(embed_dim, hidden_size, num_layers=num_layers,\n",
    "                              nonlinearity=\"tanh\" if activation != \"relu\" else \"relu\",\n",
    "                              dropout=dropout if num_layers > 1 else 0.0,\n",
    "                              bidirectional=False, batch_first=True)\n",
    "        elif rnn_type == \"lstm\":\n",
    "            self.rnn = nn.LSTM(embed_dim, hidden_size, num_layers=num_layers,\n",
    "                               dropout=dropout if num_layers > 1 else 0.0,\n",
    "                               bidirectional=False, batch_first=True)\n",
    "        elif rnn_type == \"bilstm\":\n",
    "            self.rnn = nn.LSTM(embed_dim, hidden_size, num_layers=num_layers,\n",
    "                               dropout=dropout if num_layers > 1 else 0.0,\n",
    "                               bidirectional=True, batch_first=True)\n",
    "            bidirectional = True\n",
    "        else:\n",
    "            raise ValueError(\"rnn_type must be 'rnn', 'lstm', or 'bilstm'.\")\n",
    "\n",
    "        self.bidirectional = bidirectional\n",
    "        rnn_out_dim = hidden_size * (2 if bidirectional else 1)\n",
    "\n",
    "        # Simple pooling over time (mean-pool), then activation and output\n",
    "        self.activation = ACTIVATIONS[activation]\n",
    "        self.fc = nn.Linear(rnn_out_dim, 1)  \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embed(x)                          \n",
    "        out, _ = self.rnn(emb)                       \n",
    "        pooled = out.mean(dim=1)                     \n",
    "        h = self.dropout(self.activation(pooled))    \n",
    "        logits = self.fc(h).squeeze(1)               \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval utilities (BCE loss, accuracy, macro-F1, gradient clipping option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    vocab_size: int\n",
    "    seq_len: int = 50\n",
    "    rnn_type: str = \"lstm\"          \n",
    "    activation: str = \"relu\"        \n",
    "    optimizer: str = \"adam\"         \n",
    "    embed_dim: int = 100\n",
    "    hidden_size: int = 64\n",
    "    num_layers: int = 2\n",
    "    dropout: float = 0.5\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 5\n",
    "    grad_clip_max_norm: float | None = None  \n",
    "\n",
    "def make_optimizer(name: str, params, lr=1e-3):\n",
    "    name = name.lower()\n",
    "    if name == \"adam\":    return torch.optim.Adam(params, lr=lr)\n",
    "    if name == \"sgd\":     return torch.optim.SGD(params, lr=lr, momentum=0.9)\n",
    "    if name == \"rmsprop\": return torch.optim.RMSprop(params, lr=lr, momentum=0.9)\n",
    "    raise ValueError(\"optimizer must be 'adam', 'sgd', or 'rmsprop'\")\n",
    "\n",
    "def epoch_loop(model, loader, optimizer=None, clip_max_norm=None):\n",
    "    is_train = optimizer is not None\n",
    "    model.train(mode=is_train)\n",
    "    losses, y_true, y_pred = [], [], []\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    start = time.time()\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "\n",
    "        if is_train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            if clip_max_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_max_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "        y_pred.extend((probs >= 0.5).astype(int).tolist())\n",
    "        y_true.extend(yb.detach().cpu().numpy().astype(int).tolist())\n",
    "\n",
    "    dur = time.time() - start\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1m = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    return np.mean(losses), acc, f1m, dur\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# single runner + csv logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(cfg: TrainConfig):\n",
    "    # Data\n",
    "    train_loader, val_loader, test_loader = build_loaders(cfg.seq_len, batch_size=cfg.batch_size)\n",
    "\n",
    "    # Model\n",
    "    model = RNNClassifier(\n",
    "        vocab_size=cfg.vocab_size, embed_dim=cfg.embed_dim, hidden_size=cfg.hidden_size,\n",
    "        num_layers=cfg.num_layers, dropout=cfg.dropout,\n",
    "        rnn_type=cfg.rnn_type if cfg.rnn_type != \"bilstm\" else \"bilstm\",\n",
    "        activation=cfg.activation, bidirectional=(cfg.rnn_type == \"bilstm\")\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    optimizer = make_optimizer(cfg.optimizer, model.parameters())\n",
    "\n",
    "    # Train\n",
    "    best_val = float(\"inf\")\n",
    "    history = []\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        tr_loss, tr_acc, tr_f1, tr_time = epoch_loop(model, train_loader, optimizer,\n",
    "                                                     clip_max_norm=cfg.grad_clip_max_norm)\n",
    "        va_loss, va_acc, va_f1, va_time = epoch_loop(model, val_loader, optimizer=None)\n",
    "\n",
    "        history.append({\"epoch\": epoch, \"train_loss\": tr_loss, \"val_loss\": va_loss})\n",
    "        print(f\"[{epoch}/{cfg.epochs}] train_loss={tr_loss:.4f} \"\n",
    "              f\"val_loss={va_loss:.4f} val_acc={va_acc:.4f} val_f1={va_f1:.4f}\")\n",
    "\n",
    "        \n",
    "        if va_loss < best_val:\n",
    "            best_val = va_loss\n",
    "            torch.save(model.state_dict(), RESULTS / \"tmp_best.pt\")\n",
    "\n",
    "    # Loading best and evaluating on test\n",
    "    model.load_state_dict(torch.load(RESULTS / \"tmp_best.pt\", map_location=DEVICE))\n",
    "    te_loss, te_acc, te_f1, te_time = epoch_loop(model, test_loader, optimizer=None)\n",
    "\n",
    "    # Log one consolidated line to metrics.csv\n",
    "    row = {\n",
    "        \"model\": cfg.rnn_type.upper(),\n",
    "        \"activation\": cfg.activation,\n",
    "        \"optimizer\": cfg.optimizer.upper(),\n",
    "        \"seq_len\": cfg.seq_len,\n",
    "        \"grad_clip\": cfg.grad_clip_max_norm if cfg.grad_clip_max_norm is not None else \"off\",\n",
    "        \"epochs\": cfg.epochs,\n",
    "        \"final_test_loss\": round(te_loss, 6),\n",
    "        \"accuracy\": round(te_acc, 6),\n",
    "        \"f1_macro\": round(te_f1, 6),\n",
    "        \"epoch_time_s(last_train)\": round(history[-1][\"train_loss\"] * 0 + te_time, 4),  # placeholder: per-epoch measured above in epoch_loop\n",
    "        \"device\": str(DEVICE),\n",
    "        \"seed\": SEED,\n",
    "    }\n",
    "    pd.DataFrame([row]).to_csv(METRICS_CSV, mode=\"a\", header=not METRICS_CSV.exists(), index=False)\n",
    "\n",
    "    \n",
    "    pd.DataFrame(history).to_csv(RESULTS / f\"loss_curve_{cfg.rnn_type}_{cfg.activation}_{cfg.optimizer}_len{cfg.seq_len}_clip{row['grad_clip']}.csv\",\n",
    "                                 index=False)\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirming the vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DATA_DIR = /Users/ayushgaur/Desktop/NLP/HW3/Data/processed\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"/Users/ayushgaur/Desktop/NLP/HW3/Data/processed\")\n",
    "assert (DATA_DIR / \"vocab.json\").exists(), f\"Not found: {DATA_DIR/'vocab.json'}\"\n",
    "print(\"Using DATA_DIR =\", DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vocab_json = DATA_DIR / \"vocab.json\"\n",
    "with open(vocab_json) as f:\n",
    "    word2id = json.load(f)\n",
    "VOCAB_SIZE = max(map(int, word2id.values())) + 1\n",
    "VOCAB_SIZE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal matrix covering the slide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/5] train_loss=0.6367 val_loss=0.5500 val_acc=0.7192 val_f1=0.7188\n",
      "[2/5] train_loss=0.5015 val_loss=0.5085 val_acc=0.7460 val_f1=0.7447\n",
      "[3/5] train_loss=0.4329 val_loss=0.4865 val_acc=0.7708 val_f1=0.7702\n",
      "[4/5] train_loss=0.3800 val_loss=0.4919 val_acc=0.7708 val_f1=0.7706\n",
      "[5/5] train_loss=0.3370 val_loss=0.4861 val_acc=0.7696 val_f1=0.7692\n",
      "[1/5] train_loss=0.6062 val_loss=0.5299 val_acc=0.7248 val_f1=0.7225\n",
      "[2/5] train_loss=0.4685 val_loss=0.4937 val_acc=0.7584 val_f1=0.7573\n",
      "[3/5] train_loss=0.3914 val_loss=0.4847 val_acc=0.7776 val_f1=0.7776\n",
      "[4/5] train_loss=0.3257 val_loss=0.5543 val_acc=0.7564 val_f1=0.7539\n",
      "[5/5] train_loss=0.2643 val_loss=0.6317 val_acc=0.7696 val_f1=0.7696\n",
      "[1/5] train_loss=0.6001 val_loss=0.5229 val_acc=0.7376 val_f1=0.7356\n",
      "[2/5] train_loss=0.4545 val_loss=0.4839 val_acc=0.7688 val_f1=0.7686\n",
      "[3/5] train_loss=0.3681 val_loss=0.5054 val_acc=0.7696 val_f1=0.7690\n",
      "[4/5] train_loss=0.2913 val_loss=0.5055 val_acc=0.7768 val_f1=0.7767\n",
      "[5/5] train_loss=0.2177 val_loss=0.5835 val_acc=0.7668 val_f1=0.7663\n",
      "[1/5] train_loss=0.6697 val_loss=0.6153 val_acc=0.6816 val_f1=0.6816\n",
      "[2/5] train_loss=0.5771 val_loss=0.5455 val_acc=0.7232 val_f1=0.7185\n",
      "[3/5] train_loss=0.4970 val_loss=0.5149 val_acc=0.7544 val_f1=0.7539\n",
      "[4/5] train_loss=0.4377 val_loss=0.5202 val_acc=0.7448 val_f1=0.7442\n",
      "[5/5] train_loss=0.3908 val_loss=0.5328 val_acc=0.7440 val_f1=0.7437\n",
      "[1/5] train_loss=0.6025 val_loss=0.5216 val_acc=0.7360 val_f1=0.7351\n",
      "[2/5] train_loss=0.4693 val_loss=0.4773 val_acc=0.7652 val_f1=0.7652\n",
      "[3/5] train_loss=0.3934 val_loss=0.4766 val_acc=0.7616 val_f1=0.7611\n",
      "[4/5] train_loss=0.3221 val_loss=0.5074 val_acc=0.7688 val_f1=0.7687\n",
      "[5/5] train_loss=0.2540 val_loss=0.5996 val_acc=0.7704 val_f1=0.7698\n",
      "[1/5] train_loss=0.5983 val_loss=0.5231 val_acc=0.7272 val_f1=0.7267\n",
      "[2/5] train_loss=0.4646 val_loss=0.4911 val_acc=0.7544 val_f1=0.7544\n",
      "[3/5] train_loss=0.3840 val_loss=0.4901 val_acc=0.7564 val_f1=0.7560\n",
      "[4/5] train_loss=0.3127 val_loss=0.5486 val_acc=0.7560 val_f1=0.7547\n",
      "[5/5] train_loss=0.2448 val_loss=0.6128 val_acc=0.7556 val_f1=0.7556\n",
      "[1/5] train_loss=0.6137 val_loss=0.5347 val_acc=0.7228 val_f1=0.7212\n",
      "[2/5] train_loss=0.4766 val_loss=0.4891 val_acc=0.7620 val_f1=0.7620\n",
      "[3/5] train_loss=0.3960 val_loss=0.5007 val_acc=0.7596 val_f1=0.7596\n",
      "[4/5] train_loss=0.3253 val_loss=0.5228 val_acc=0.7652 val_f1=0.7651\n",
      "[5/5] train_loss=0.2554 val_loss=0.6294 val_acc=0.7536 val_f1=0.7534\n",
      "[1/5] train_loss=0.6932 val_loss=0.6931 val_acc=0.5200 val_f1=0.4935\n",
      "[2/5] train_loss=0.6931 val_loss=0.6930 val_acc=0.5204 val_f1=0.4360\n",
      "[3/5] train_loss=0.6931 val_loss=0.6930 val_acc=0.5204 val_f1=0.4552\n",
      "[4/5] train_loss=0.6931 val_loss=0.6930 val_acc=0.5164 val_f1=0.4774\n",
      "[5/5] train_loss=0.6930 val_loss=0.6929 val_acc=0.5044 val_f1=0.3353\n",
      "[1/5] train_loss=0.5605 val_loss=0.4827 val_acc=0.7592 val_f1=0.7573\n",
      "[2/5] train_loss=0.4100 val_loss=0.5063 val_acc=0.7604 val_f1=0.7601\n",
      "[3/5] train_loss=0.3422 val_loss=0.5467 val_acc=0.7696 val_f1=0.7695\n",
      "[4/5] train_loss=0.2973 val_loss=0.5355 val_acc=0.7644 val_f1=0.7633\n",
      "[5/5] train_loss=0.2629 val_loss=0.5857 val_acc=0.7592 val_f1=0.7586\n",
      "[1/5] train_loss=0.6380 val_loss=0.5931 val_acc=0.6764 val_f1=0.6720\n",
      "[2/5] train_loss=0.5386 val_loss=0.5572 val_acc=0.7152 val_f1=0.7150\n",
      "[3/5] train_loss=0.4697 val_loss=0.5775 val_acc=0.7132 val_f1=0.7120\n",
      "[4/5] train_loss=0.3992 val_loss=0.6021 val_acc=0.7144 val_f1=0.7143\n",
      "[5/5] train_loss=0.3327 val_loss=0.6751 val_acc=0.7076 val_f1=0.7065\n",
      "[1/5] train_loss=0.6055 val_loss=0.5288 val_acc=0.7332 val_f1=0.7330\n",
      "[2/5] train_loss=0.4686 val_loss=0.4984 val_acc=0.7516 val_f1=0.7514\n",
      "[3/5] train_loss=0.3911 val_loss=0.5061 val_acc=0.7564 val_f1=0.7564\n",
      "[4/5] train_loss=0.3216 val_loss=0.5576 val_acc=0.7540 val_f1=0.7531\n",
      "[5/5] train_loss=0.2590 val_loss=0.6066 val_acc=0.7600 val_f1=0.7597\n",
      "[1/5] train_loss=0.5582 val_loss=0.4661 val_acc=0.7804 val_f1=0.7790\n",
      "[2/5] train_loss=0.3986 val_loss=0.3960 val_acc=0.8236 val_f1=0.8236\n",
      "[3/5] train_loss=0.3177 val_loss=0.3932 val_acc=0.8204 val_f1=0.8202\n",
      "[4/5] train_loss=0.2581 val_loss=0.4122 val_acc=0.8216 val_f1=0.8216\n",
      "[5/5] train_loss=0.2040 val_loss=0.4506 val_acc=0.8252 val_f1=0.8251\n",
      "[1/5] train_loss=0.6107 val_loss=0.5311 val_acc=0.7336 val_f1=0.7324\n",
      "[2/5] train_loss=0.4711 val_loss=0.4974 val_acc=0.7548 val_f1=0.7535\n",
      "[3/5] train_loss=0.3942 val_loss=0.4999 val_acc=0.7600 val_f1=0.7587\n",
      "[4/5] train_loss=0.3244 val_loss=0.5102 val_acc=0.7644 val_f1=0.7644\n",
      "[5/5] train_loss=0.2584 val_loss=0.5964 val_acc=0.7632 val_f1=0.7630\n",
      "[1/5] train_loss=0.6038 val_loss=0.5375 val_acc=0.7240 val_f1=0.7220\n",
      "[2/5] train_loss=0.4734 val_loss=0.5116 val_acc=0.7520 val_f1=0.7506\n",
      "[3/5] train_loss=0.3897 val_loss=0.5051 val_acc=0.7608 val_f1=0.7590\n",
      "[4/5] train_loss=0.3211 val_loss=0.5447 val_acc=0.7616 val_f1=0.7616\n",
      "[5/5] train_loss=0.2532 val_loss=0.5704 val_acc=0.7548 val_f1=0.7544\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>activation</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>grad_clip</th>\n",
       "      <th>epochs</th>\n",
       "      <th>final_test_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>epoch_time_s(last_train)</th>\n",
       "      <th>device</th>\n",
       "      <th>seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RNN</td>\n",
       "      <td>relu</td>\n",
       "      <td>ADAM</td>\n",
       "      <td>50</td>\n",
       "      <td>off</td>\n",
       "      <td>5</td>\n",
       "      <td>0.480314</td>\n",
       "      <td>0.76792</td>\n",
       "      <td>0.767447</td>\n",
       "      <td>1.6807</td>\n",
       "      <td>cpu</td>\n",
       "      <td>1337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>relu</td>\n",
       "      <td>ADAM</td>\n",
       "      <td>50</td>\n",
       "      <td>off</td>\n",
       "      <td>5</td>\n",
       "      <td>0.490834</td>\n",
       "      <td>0.76404</td>\n",
       "      <td>0.764038</td>\n",
       "      <td>6.3161</td>\n",
       "      <td>cpu</td>\n",
       "      <td>1337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BILSTM</td>\n",
       "      <td>relu</td>\n",
       "      <td>ADAM</td>\n",
       "      <td>50</td>\n",
       "      <td>off</td>\n",
       "      <td>5</td>\n",
       "      <td>0.487694</td>\n",
       "      <td>0.76188</td>\n",
       "      <td>0.761685</td>\n",
       "      <td>13.6662</td>\n",
       "      <td>cpu</td>\n",
       "      <td>1337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>ADAM</td>\n",
       "      <td>50</td>\n",
       "      <td>off</td>\n",
       "      <td>5</td>\n",
       "      <td>0.508847</td>\n",
       "      <td>0.75428</td>\n",
       "      <td>0.753633</td>\n",
       "      <td>6.6786</td>\n",
       "      <td>cpu</td>\n",
       "      <td>1337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>relu</td>\n",
       "      <td>ADAM</td>\n",
       "      <td>50</td>\n",
       "      <td>off</td>\n",
       "      <td>5</td>\n",
       "      <td>0.481788</td>\n",
       "      <td>0.76440</td>\n",
       "      <td>0.763800</td>\n",
       "      <td>6.5080</td>\n",
       "      <td>cpu</td>\n",
       "      <td>1337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>tanh</td>\n",
       "      <td>ADAM</td>\n",
       "      <td>50</td>\n",
       "      <td>off</td>\n",
       "      <td>5</td>\n",
       "      <td>0.484845</td>\n",
       "      <td>0.76512</td>\n",
       "      <td>0.764925</td>\n",
       "      <td>6.0606</td>\n",
       "      <td>cpu</td>\n",
       "      <td>1337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>relu</td>\n",
       "      <td>ADAM</td>\n",
       "      <td>50</td>\n",
       "      <td>off</td>\n",
       "      <td>5</td>\n",
       "      <td>0.488589</td>\n",
       "      <td>0.75704</td>\n",
       "      <td>0.757032</td>\n",
       "      <td>6.4766</td>\n",
       "      <td>cpu</td>\n",
       "      <td>1337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>relu</td>\n",
       "      <td>SGD</td>\n",
       "      <td>50</td>\n",
       "      <td>off</td>\n",
       "      <td>5</td>\n",
       "      <td>0.692958</td>\n",
       "      <td>0.49916</td>\n",
       "      <td>0.334303</td>\n",
       "      <td>6.0823</td>\n",
       "      <td>cpu</td>\n",
       "      <td>1337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>relu</td>\n",
       "      <td>RMSPROP</td>\n",
       "      <td>50</td>\n",
       "      <td>off</td>\n",
       "      <td>5</td>\n",
       "      <td>0.486625</td>\n",
       "      <td>0.75968</td>\n",
       "      <td>0.757985</td>\n",
       "      <td>6.1206</td>\n",
       "      <td>cpu</td>\n",
       "      <td>1337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>relu</td>\n",
       "      <td>ADAM</td>\n",
       "      <td>25</td>\n",
       "      <td>off</td>\n",
       "      <td>5</td>\n",
       "      <td>0.559067</td>\n",
       "      <td>0.70484</td>\n",
       "      <td>0.704575</td>\n",
       "      <td>3.1427</td>\n",
       "      <td>cpu</td>\n",
       "      <td>1337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>relu</td>\n",
       "      <td>ADAM</td>\n",
       "      <td>50</td>\n",
       "      <td>off</td>\n",
       "      <td>5</td>\n",
       "      <td>0.484411</td>\n",
       "      <td>0.76384</td>\n",
       "      <td>0.763437</td>\n",
       "      <td>6.2108</td>\n",
       "      <td>cpu</td>\n",
       "      <td>1337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>relu</td>\n",
       "      <td>ADAM</td>\n",
       "      <td>100</td>\n",
       "      <td>off</td>\n",
       "      <td>5</td>\n",
       "      <td>0.396150</td>\n",
       "      <td>0.82272</td>\n",
       "      <td>0.822396</td>\n",
       "      <td>12.3214</td>\n",
       "      <td>cpu</td>\n",
       "      <td>1337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>relu</td>\n",
       "      <td>ADAM</td>\n",
       "      <td>50</td>\n",
       "      <td>off</td>\n",
       "      <td>5</td>\n",
       "      <td>0.501528</td>\n",
       "      <td>0.75160</td>\n",
       "      <td>0.750061</td>\n",
       "      <td>6.2651</td>\n",
       "      <td>cpu</td>\n",
       "      <td>1337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>relu</td>\n",
       "      <td>ADAM</td>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.493700</td>\n",
       "      <td>0.76196</td>\n",
       "      <td>0.760032</td>\n",
       "      <td>6.3978</td>\n",
       "      <td>cpu</td>\n",
       "      <td>1337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     model activation optimizer  seq_len grad_clip  epochs  final_test_loss  \\\n",
       "0      RNN       relu      ADAM       50       off       5         0.480314   \n",
       "1     LSTM       relu      ADAM       50       off       5         0.490834   \n",
       "2   BILSTM       relu      ADAM       50       off       5         0.487694   \n",
       "3     LSTM    sigmoid      ADAM       50       off       5         0.508847   \n",
       "4     LSTM       relu      ADAM       50       off       5         0.481788   \n",
       "5     LSTM       tanh      ADAM       50       off       5         0.484845   \n",
       "6     LSTM       relu      ADAM       50       off       5         0.488589   \n",
       "7     LSTM       relu       SGD       50       off       5         0.692958   \n",
       "8     LSTM       relu   RMSPROP       50       off       5         0.486625   \n",
       "9     LSTM       relu      ADAM       25       off       5         0.559067   \n",
       "10    LSTM       relu      ADAM       50       off       5         0.484411   \n",
       "11    LSTM       relu      ADAM      100       off       5         0.396150   \n",
       "12    LSTM       relu      ADAM       50       off       5         0.501528   \n",
       "13    LSTM       relu      ADAM       50       1.0       5         0.493700   \n",
       "\n",
       "    accuracy  f1_macro  epoch_time_s(last_train) device  seed  \n",
       "0    0.76792  0.767447                    1.6807    cpu  1337  \n",
       "1    0.76404  0.764038                    6.3161    cpu  1337  \n",
       "2    0.76188  0.761685                   13.6662    cpu  1337  \n",
       "3    0.75428  0.753633                    6.6786    cpu  1337  \n",
       "4    0.76440  0.763800                    6.5080    cpu  1337  \n",
       "5    0.76512  0.764925                    6.0606    cpu  1337  \n",
       "6    0.75704  0.757032                    6.4766    cpu  1337  \n",
       "7    0.49916  0.334303                    6.0823    cpu  1337  \n",
       "8    0.75968  0.757985                    6.1206    cpu  1337  \n",
       "9    0.70484  0.704575                    3.1427    cpu  1337  \n",
       "10   0.76384  0.763437                    6.2108    cpu  1337  \n",
       "11   0.82272  0.822396                   12.3214    cpu  1337  \n",
       "12   0.75160  0.750061                    6.2651    cpu  1337  \n",
       "13   0.76196  0.760032                    6.3978    cpu  1337  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline (lstm,relu,adam) while varying factors\n",
    "BASE = dict(\n",
    "    vocab_size=VOCAB_SIZE, seq_len=50, rnn_type=\"lstm\",\n",
    "    activation=\"relu\", optimizer=\"adam\",\n",
    "    embed_dim=100, hidden_size=64, num_layers=2, dropout=0.5,\n",
    "    batch_size=32, epochs=5, grad_clip_max_norm=None\n",
    ")\n",
    "\n",
    "results = []\n",
    "\n",
    "#  Architectures: RNN, LSTM, BiLSTM \n",
    "for arch in [\"rnn\", \"lstm\", \"bilstm\"]:\n",
    "    cfg = TrainConfig(**{**BASE, \"rnn_type\": arch})\n",
    "    results.append(run_experiment(cfg))\n",
    "\n",
    "#  Activations: Sigmoid, ReLU, Tanh (LSTM fixed)\n",
    "for act in [\"sigmoid\", \"relu\", \"tanh\"]:\n",
    "    cfg = TrainConfig(**{**BASE, \"activation\": act, \"rnn_type\": \"lstm\"})\n",
    "    results.append(run_experiment(cfg))\n",
    "\n",
    "#  Optimizers: Adam, SGD, RMSProp (LSTM + ReLU fixed)\n",
    "for opt in [\"adam\", \"sgd\", \"rmsprop\"]:\n",
    "    cfg = TrainConfig(**{**BASE, \"optimizer\": opt, \"rnn_type\": \"lstm\", \"activation\": \"relu\"})\n",
    "    results.append(run_experiment(cfg))\n",
    "\n",
    "#  Sequence length: 25, 50, 100 (LSTM + ReLU + Adam fixed)\n",
    "for L in [25, 50, 100]:\n",
    "    cfg = TrainConfig(**{**BASE, \"seq_len\": L, \"rnn_type\": \"lstm\", \"activation\": \"relu\", \"optimizer\": \"adam\"})\n",
    "    results.append(run_experiment(cfg))\n",
    "\n",
    "#  Gradient clipping: off vs on (e.g., max_norm=1.0) with a representative model\n",
    "for clip in [None, 1.0]:\n",
    "    cfg = TrainConfig(**{**BASE, \"grad_clip_max_norm\": clip, \"rnn_type\": \"lstm\", \"activation\": \"relu\", \"optimizer\": \"adam\"})\n",
    "    results.append(run_experiment(cfg))\n",
    "\n",
    "pd.DataFrame(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
